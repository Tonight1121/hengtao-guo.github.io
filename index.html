<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hengtao Guo</title>

  <meta name="author" content="Hengtao Guo">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Hengtao Guo
                  </p>
                  <p style="text-align:center">
                    <!-- <a href="mailto:jonbarron@gmail.com">Email</a> &nbsp;/&nbsp; -->
                    <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                    <a href="https://scholar.google.com/citations?user=MiOqrWkAAAAJ&hl=zh-CN">Google Scholar</a>
                    &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/hengtao-guo-ph-d-a1289a155/">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://www.researchgate.net/profile/Hengtao-Guo">Research Gate</a> &nbsp;/&nbsp;
                    <a href="https://github.com/Tonight1121">Github</a>
                  </p>
                  <p>I am a Machine Learning Software Engineer at <a href="https://about.google/">Google</a>, where I
                    focus on building recommendation systems in YouTube main app. Before joining Google, I have worked
                    as a Research Intern at <a href="https://www.uii-ai.com/">UII America</a>.
                  </p>
                  <p>I received my Ph.D. in Biomedical Engineering from <a href="https://www.rpi.edu/">Rensselaer
                      Polytechnic Institute</a> in 2022, and B.S. in Computer Science from <a
                      href="https://en.whu.edu.cn/">Wuhan
                      University</a> in 2018.
                  </p>

                </td>
                <td style="padding:2.5%;width:30%;max-width:30%">
                  <a href="images/hengtaoguo.jpeg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/hengtaoguo.jpeg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research Fields</h2>
                  <p>
                    My research focuses span across machine learning, deep learning, and generative artificial
                    intelligence:
                  </p>
                  <ul>
                    <li><b>Computer Vision</b>: Medical image analysis, image registration/segmentation, ultrasound
                      reconstruction</li>
                    <li><b>Recommendation Systems</b>: User modeling, video/audio representation learning
                    <li><b>Natural Language Processing</b>: Large language models, retrival-augmented generation
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Publications</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/SMPL-a/sliding_axial_view.gif' width=110%>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a
                    href="https://openaccess.thecvf.com/content/CVPR2022/html/Guo_SMPL-A_Modeling_Person-Specific_Deformable_Anatomy_CVPR_2022_paper.html">
                    <span class="papertitle">SMPL-A: Modeling Person-Specific Deformable Anatomy</span>
                  </a>
                  <br>
                  <strong>Hengtao Guo</strong>,
                  <a href="https://planche.me/">Benjamin Planche</a>,
                  <a href="https://mzhengrpi.github.io/">Meng Zheng</a>,
                  <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
                  <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>,
                  <a href="http://wuziyan.com/">Ziyan Wu</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (CVPR), 2022
                  <br>
                  <p></p>
                  <p>
                    We propose a learning-based method that uses medical scans to predict internal organ deformation
                    across various human poses, aiding radiotherapy and similar treatments. This approach builds a
                    patient-specific model encoding the organ's shape and elasticity, allowing for deformation
                    estimation based on the patient's current pose. This innovation offers clinicians precise guidance
                    without additional scans or procedures.
                  </p>
                  <a href="data/smpla/DigitalAnatomy-Poster-ver3.pdf">poster</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/DCL-Net/volume.jpg' width=110%>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://link.springer.com/chapter/10.1007/978-3-030-59716-0_44">
                    <span class="papertitle">Sensorless Freehand 3D Ultrasound Reconstruction via Deep Contextual
                      Learning</span>
                  </a>
                  <br>
                  <strong>Hengtao Guo</strong>,
                  <a href="https://ieeexplore.ieee.org/author/37395027100">Sheng Xu</a>,
                  <a href="https://scholar.google.co.uk/citations?user=5hOjrW8AAAAJ&hl=en&oi=ao">Bradford J. Wood</a>,
                  <a href="https://scholar.google.com/citations?user=qkIp7AEAAAAJ&hl=en&oi=ao">Pingkun Yan</a>
                  <br>
                  <em>Medical Image Computing and Computer Assisted Intervention</em> (MICCAI), 2020
                  <br>
                  <p></p>
                  <p>
                    We propose a deep contextual learning network (DCL-Net), which can efficiently exploit the image
                    feature relationship between US frames and reconstruct 3D US volumes without any tracking device.
                    The proposed DCL-Net utilizes 3D convolutions over a US video segment for feature extraction. An
                    embedded self-attention module makes the network focus on the speckle-rich areas for better spatial
                    movement prediction, with a novel case-wise correlation loss to stabilize the training
                    process for improved accuracy.
                  </p>
                  <a href="https://arxiv.org/abs/2006.07694">arxiv</a>
                  /
                  <a href="https://github.com/DIAL-RPI/FreehandUSRecon">code</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/USRecon/recon_vol.gif' width=110%>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://https://ieeexplore.ieee.org/abstract/document/9891829">
                    <span class="papertitle">Ultrasound Volume Reconstruction From Freehand Scans Without
                      Tracking</span>
                  </a>
                  <br>
                  <strong>Hengtao Guo</strong>,
                  <a href="https://scholar.google.com/citations?user=P4Y_RSgAAAAJ&hl=en&oi=ao">Hanqing Chao</a>,
                  <a href="https://ieeexplore.ieee.org/author/37395027100">Sheng Xu</a>,
                  <a href="https://scholar.google.co.uk/citations?user=5hOjrW8AAAAJ&hl=en&oi=ao">Bradford J. Wood</a>,
                  <a href="https://scholar.google.com/citations?user=8UYmQqkAAAAJ&hl=en&oi=sra">Jing Wang</a>,
                  <a href="https://scholar.google.com/citations?user=qkIp7AEAAAAJ&hl=en&oi=ao">Pingkun Yan</a>
                  <br>
                  <em>IEEE Transactions on Biomedical Engineering</em>, 2022
                  <br>
                  <p></p>
                  <p>
                    We propose a deep contextual-contrastive network, utilizing self-attention to focus on the
                    speckle-rich areas to estimate spatial movement and then minimizes a margin ranking loss for
                    contrastive feature learning. We train and validate the model on two independent datasets, evaluated
                    with including the DICE of reconstructed organ segmentation.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/KAMP/KAMP.jpg' width=110%>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7007835/">
                    <span class="papertitle">Knowledge-based Analysis for Mortality Prediction from CT Images</span>
                  </a>
                  <br>
                  <strong>Hengtao Guo</strong>,
                  <a href="https://scholar.google.com/citations?user=9WSsCZkAAAAJ&hl=en&oi=ao">Uwe Kruger</a>,
                  <a href="https://scholar.google.com/citations?user=pjK2mQwAAAAJ&hl=en&oi=ao">Ge Wang</a>,
                  <a href="https://www.massgeneral.org/doctors/17961/mannudeep-kalra">Mannudeep K. Kalra</a>,
                  <a href="https://scholar.google.com/citations?user=qkIp7AEAAAAJ&hl=en&oi=ao">Pingkun Yan</a>
                  <br>
                  <em>IEEE Journal of Biomedical and Health Informatics</em>, 2019
                  <br>
                  <p></p>
                  <p>
                    This paper introduces a knowledge-based analytical method using deep convolutional neural network
                    (CNN) for all-cause mortality prediction. The underlying approach combines structural image features
                    extracted from CNNs, based on LDCT volume at different scales, and clinical knowledge obtained from
                    quantitative measurements, to predict the mortality risk of lung cancer screening subjects.
                  </p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/FVReg/gif_pd_3.gif' width=110%>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9989409">
                    <span class="papertitle">Ultrasound Frame-to-Volume Registration via Deep Learning for
                      Interventional Guidance</span>
                  </a>
                  <br>
                  <strong>Hengtao Guo</strong>,
                  <a href="https://scholar.google.com/citations?user=-wbE0RQAAAAJ&hl=en&oi=ao">Xuanang Xu</a>,
                  <a href="https://scholar.google.com/citations?user=ms_5p0sAAAAJ&hl=en&oi=ao">Xinrui Song</a>,
                  <a href="https://ieeexplore.ieee.org/author/37395027100">Sheng Xu</a>,
                  <a href="https://scholar.google.com/citations?user=P4Y_RSgAAAAJ&hl=en&oi=ao">Hanqing Chao</a>,
                  <a href="https://dial.rpi.edu/people/joshua-myers">Joshua Myers</a>,
                  <a href="https://scholar.google.com/citations?user=XiMbUboAAAAJ&hl=en&oi=sra">Baris Turkbey</a>,
                  <a href="https://irp.nih.gov/pi/peter-pinto">Peter A. Pinto</a>,
                  <a href="https://scholar.google.co.uk/citations?user=5hOjrW8AAAAJ&hl=en&oi=ao">Bradford J. Wood</a>,
                  <a href="https://scholar.google.com/citations?user=qkIp7AEAAAAJ&hl=en&oi=ao">Pingkun Yan</a>
                  <br>
                  <em>IEEE Journal of Biomedical and Health Informatics</em>, 2019
                  <br>
                  <p></p>
                  <p>
                    We propose a novel US frame-to-volume registration (FVReg) pipeline to bridge the dimensionality gap
                    between 2-D US frames and 3-D US volume. The developed pipeline is implemented using deep neural
                    networks, which are fully automatic without requiring external tracking devices. We validated our
                    method on a clinical dataset with 618 subjects and tested its potential on real-time 2-D-US to
                    3-D-MR fusion navigation tasks. The proposed FVReg achieved an average target navigation error of
                    1.93 mm at 5 to 14 fps.
                  </p>
                  <a href="https://github.com/DIAL-RPI/Frame-to-Volume-Registration">code</a>
                </td>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/Tri2D-Net/attention.JPEG' width=110%>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://www.nature.com/articles/s41467-021-23235-4">
                    <span class="papertitle">Deep learning predicts cardiovascular disease risks from lung cancer
                      screening low dose computed tomography</span>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=P4Y_RSgAAAAJ&hl=en&oi=ao">Hanqing Chao</a>,
                  <a href="https://scholar.google.com/citations?user=RYfSzKwAAAAJ&hl=en&oi=ao">Hongming Shan</a>,
                  <a href="https://scholar.google.com/citations?user=kek-c3wAAAAJ&hl=en&oi=ao">Fatemeh Homayounieh</a>,
                  <a href="https://scholar.google.com/citations?user=286ZiX4AAAAJ&hl=en&oi=sra">Ramandeep Singh</a>,
                  <a href="https://scholar.google.com/citations?user=mWUFHzcAAAAJ&hl=en&oi=sra">Ruhani Doda Khera</a>,
                  <strong>Hengtao Guo</strong>,
                  <a href="https://www.nature.com/search?author=Timothy%20Su">Timothy Su</a>,
                  <a href="https://scholar.google.com/citations?user=pjK2mQwAAAAJ&hl=en&oi=ao">Ge Wang</a>,
                  <a href="https://www.massgeneral.org/doctors/17961/mannudeep-kalra">Mannudeep K. Kalra</a>,
                  <a href="https://scholar.google.com/citations?user=qkIp7AEAAAAJ&hl=en&oi=ao">Pingkun Yan</a>
                  <br>
                  <em>Nature Communications</em>, 2021
                  <br>
                  <p></p>
                  <p>
                    Cancer patients have a higher risk of cardiovascular disease (CVD) mortality than the general
                    population. Low dose computed tomography (LDCT) for lung cancer screening offers an opportunity for
                    simultaneous CVD risk estimation in at-risk patients. Our deep learning CVD risk prediction model,
                    trained with 30,286 LDCTs from the National Lung Cancer Screening Trial, achieves an area under the
                    curve (AUC) of 0.871 on a separate test set of 2,085 subjects and identifies patients with high CVD
                    mortality risks (AUC of 0.768).
                  </p>
                </td>
              </tr>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>