<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hengtao Guo</title>

  <meta name="author" content="Hengtao Guo">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Hengtao Guo
                  </p>
                  <p style="text-align:center">
                    <!-- <a href="mailto:jonbarron@gmail.com">Email</a> &nbsp;/&nbsp; -->
                    <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                    <a href="https://scholar.google.com/citations?user=MiOqrWkAAAAJ&hl=zh-CN">Google Scholar</a>
                    &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/hengtao-guo-ph-d-a1289a155/">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://www.researchgate.net/profile/Hengtao-Guo">Research Gate</a> &nbsp;/&nbsp;
                    <a href="https://github.com/Tonight1121">Github</a>
                  </p>
                  <p>I am a Machine Learning Software Engineer at <a href="https://about.google/">Google</a>, where I
                    focus on building recommendation systems in YouTube main app. Before joining Google, I have worked
                    as a Research Intern at <a href="https://www.uii-ai.com/">UII America</a>.
                  </p>
                  <p>I received my Ph.D. in Biomedical Engineering from <a href="https://www.rpi.edu/">Rensselaer
                      Polytechnic Institute</a> in 2022, and B.S. in Computer Science from <a
                      href="https://en.whu.edu.cn/">Wuhan
                      University</a> in 2018.
                  </p>

                </td>
                <td style="padding:2.5%;width:30%;max-width:30%">
                  <a href="images/hengtaoguo.jpeg"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/hengtaoguo.jpeg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research Fields</h2>
                  <p>
                    My research focuses span across machine learning, deep learning, and generative artificial
                    intelligence:
                  </p>
                  <ul>
                    <li><b>Computer Vision</b>: Medical image analysis, image registration/segmentation, ultrasound
                      reconstruction</li>
                    <li><b>Recommendation Systems</b>: User modeling, video/audio representation learning
                    <li><b>Natural Language Processing</b>: Large language models, retrival-augmented generation
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Publications</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/sliding_axial_view.gif' width=110%>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a
                    href="https://openaccess.thecvf.com/content/CVPR2022/html/Guo_SMPL-A_Modeling_Person-Specific_Deformable_Anatomy_CVPR_2022_paper.html">
                    <span class="papertitle">SMPL-A: Modeling Person-Specific Deformable Anatomy</span>
                  </a>
                  <br>
                  <strong>Hengtao Guo</strong>,
                  <a href="https://planche.me/">Benjamin Planche</a>,
                  <a href="https://mzhengrpi.github.io/">Meng Zheng</a>,
                  <a href="https://karanams.github.io/">Srikrishna Karanam</a>,
                  <a href="https://scholar.google.com/citations?user=S2BT6ogAAAAJ&hl=en">Terrence Chen</a>,
                  <a href="http://wuziyan.com/">Ziyan Wu</a>
                  <br>
                  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition</em> (CVPR), 2022
                  <br>
                  <p></p>
                  <p>
                    Introducing a learning-based method that uses medical scans to predict internal organ deformation
                    across various human poses, aiding radiotherapy and similar treatments. This approach builds a
                    patient-specific model encoding the organ's shape and elasticity, allowing for deformation
                    estimation based on the patient's current pose. This innovation offers clinicians precise guidance
                    without additional scans or procedures.
                  </p>
                  <a href="data/smpla/DigitalAnatomy-Poster-ver3.pdf">poster</a>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <img src='images/DCL-Net/volume.jpg' width=110%>
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://link.springer.com/chapter/10.1007/978-3-030-59716-0_44">
                    <span class="papertitle">Sensorless Freehand 3D Ultrasound Reconstruction via Deep Contextual
                      Learning</span>
                  </a>
                  <br>
                  <strong>Hengtao Guo</strong>,
                  <a href="https://ieeexplore.ieee.org/author/37395027100">Sheng Xu</a>,
                  <a href="https://scholar.google.co.uk/citations?user=5hOjrW8AAAAJ&hl=en&oi=ao">Bradford J. Wood</a>,
                  <a href="https://scholar.google.com/citations?user=qkIp7AEAAAAJ&hl=en&oi=ao">Pingkun Yan</a>
                  <br>
                  <em>Medical Image Computing and Computer Assisted Intervention</em> (MICCAI), 2020
                  <br>
                  <p></p>
                  <p>
                    Proposing a deep contextual learning network (DCL-Net), which can efficiently exploit the image
                    feature relationship between US frames and reconstruct 3D US volumes without any tracking device.
                    The proposed DCL-Net utilizes 3D convolutions over a US video segment for feature extraction. An
                    embedded self-attention module makes the network focus on the speckle-rich areas for better spatial
                    movement prediction, with a novel case-wise correlation loss to stabilize the training
                    process for improved accuracy.
                  </p>
                  <a href="https://arxiv.org/abs/2006.07694">arxiv</a>
                  /
                  <a href="https://github.com/DIAL-RPI/FreehandUSRecon">code</a>
                </td>
              </tr>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>